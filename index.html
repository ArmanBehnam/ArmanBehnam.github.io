---
layout: default
title: Arman Behnam's website
---
<div class="blurb">
	<h1>Hi there, I'm Arman Behnam!</h1>
	<link rel="stylesheet" type="text/css" href="stylesheet.css">
	
	  <link rel="icon" type="image/jpg" href="images/photo_2021-07-16_15-42-37.jpg">
<!-- 	  <style>
	    #myimg{
	      width:100%;
	      max-width:100%;
	      border-radius:50%;
	      border: 1px solid #ddd;
	  padding: 5px;
	    }

	    p {
	      line-height: 22px;
	      font-size: 15px;
	    }

	    ul li{
	     font-size:15px;
	    }

	  </style>
	</head> -->

    <tr style="padding:0px">	
       <td style="padding:2.5%;width:63%;vertical-align:middle">

	<p>I'm a first year PhD CS student at Illinois Institute of technology. I am fortunate to be advised by <a href = "http://wangbinghui.net/">Dr. Binghui Wang</a>. 
	Now I am a reserach assistant at IIT, working on research projects dealing with Neural networks explanantion and causal inference.
	Previously, I worked as an AI product engineer at <a href = "https://tanzimyar.ir/en/">RegTech Startup-studio</a> in Tehran. 
	Before this, I also spent some fantastic time at <a href = "https://www.mse.ir/en/">Mobarakeh Steel Company</a>,  on the problem of realtime fault detection for Steel cold rolling engines bearings by deep learning methods
. 
	<p>Right now, I am on the editorial board of <a href = "https://journals.sagepub.com/editorial-board/AJL">American Journal of Lifestyle Medicine</a> summer of 2018, 
		and peer-reviewer of <a href = "https://www.springer.com/journal/10935/"/>The Journal of Primary Prevention</a>, <a href = "https://jamanetwork.com/journals/jamanetworkopen"/>JAMA Network Open</a>, and <a href = "https://www.springer.com/journal/11606/"/>Journal of General Internal Medicine</a>.</p>

        <p>I am glad to be surrounded by excellent collaborators and mentors who helped me push beyond my boundaries. I believe in fairness and truth and don't believe in TIME and SPACE. I am trying to learn how the life works, and help AI to surpass <b>LOGICAL THINKING</b>, <b">DECISION MAKING</b>, and <b">SELF-CONSCIOUSNESS</b>.</p>
	
	<p style="text-align:center">
    <a href="mailto:abehnam@hawk.iit.edu" class="button"></a>
    <a href="https://scholar.google.com/citations?user=KgzHjgMAAAAJ&hl=en&oi=ao" class="button"></a>
    <a href="https://twitter.com/Arman_behnam" class="button"></a>
    <a href="https://github.com/ArmanBehnam" class="button"></a>
    <a href="https://www.linkedin.com/in/arman-behnam/" class="button"></a>
    <a href="resume/June_2023_CV.pdf" class="button"></a></p>
	
	<p>This blog is for sharing and gaining knowledge. I will be posting about my projects, interests and the stuff I do.</p>
       </td>
    </tr>

    <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
          <td style="width:100%;vertical-align:middle">
            <heading>Research</heading>
            <p>
             	I am broadly interested in developing models that can learn from limited data and few training samples. Most of my research is developing 
		    such models for the task of Action Recognition. 
            </p>
          </td>
        </tr>
    </tbody></table>

    <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
	  <tr>
	    <td style="padding:20px;width:25%;vertical-align:middle">
	      <div class="one">
		<img src='images/strm_model.png' width="160">
	      </div>
	    </td>
	    <td style="padding:20px;width:75%;vertical-align:middle">
	      <a href="https://anirudh257.github.io/strm">
		<papertitle>Spatio-temporal Relation Modeling for Few-shot Action Recognition</papertitle>
	      </a>
	      <br>
	      <strong>Anirudh Thatipelli</strong>,
	      <a href="https://sites.google.com/view/sanath-narayan">Sanath Narayan</a>,
	      <a href="https://salman-h-khan.github.io/">Salman Khan</a>,
	      <a href = "https://scholar.google.fi/citations?user=_KlvMVoAAAAJ&hl=en">Rao Muhammad Anwer</a>,
	      <a href="https://sites.google.com/view/fahadkhans/home">Fahad Shahbaz Khan</a>,
	      <a href="https://www.bernardghanem.com/">Bernard Ghanem</a>
	      <br>
	      <strong>CVPR 2022</strong>
	      <br>
	      <a href="https://arxiv.org/abs/2112.05132">paper</a> /
	      <a href="https://github.com/Anirudh257/strm">code</a>
	      <ul>
		<li>
		  <u>Description:</u> Proposed a novel spatio-temporal enrichment module, STRM for the problem of few-shot action recognition.
		</li>
		<li>
		  <u>Outcome:</u> Improved state-of-the-art performance on the challenging dataset of SSv2 by 3.5%. 
		</li>
	      </ul>
	    </td>
	  </tr>
     </tbody></table>

    <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
	  <tr>
	    <td style="padding:20px;width:25%;vertical-align:middle">
	      <div class="one">
		<img src='images/quovadis-example.png' width="160">
	      </div>
	    </td>
	    <td style="padding:20px;width:75%;vertical-align:middle">
	      <a href="https://skeleton.iiit.ac.in/actionrecognition">
		<papertitle>Quo Vadis, Skeleton Action Recognition ?</papertitle>
	      </a>
	      <br>
	      <a href="https://pranaygupta36.github.io/">Pranay Guptan</a>,
	      <strong>Anirudh Thatipelli</strong>,
	      <a href="https://aditya2g.github.io/">Aditya Aggarwal</a>,
	      <a href = "https://in.linkedin.com/in/shubh-maheshwari-663737151">Shubh Maheshwari</a>,
	      <a href="https://researchweb.iiit.ac.in/~neel.trivedi/index.html">Neel Trivedi</a>,
	      <a href="https://fr.linkedin.com/in/sourav-das-087">Sourav Das</a>
	      <a href="https://ravika.github.io/">Ravi Kiran Sarvadevabhatla</a>
	      <br>
	      <strong>International Journal of Computer Vision (IJCV), Special Issue on Human pose, Motion, Activities and Shape in 3D, 2021</strong>
	      <br>
	      <a href="https://link.springer.com/article/10.1007/s11263-021-01470-y">paper</a> /
	      <a href="https://github.com/skelemoa/quovadis">code</a>
	      <ul>
		<li>
		  <u>Description:</u> In this work, we study current and upcoming frontiers across the landscape of skeleton-based human action recognition. We benchmark state-of-the-art models on the NTU-120 dataset and provide a multi-layered assessment.  
		</li>
		<li>
		  <u>Outcome:</u> We introduced Skeletics-152 and Skeleton-Mimetics datasets. Our results reveal the challenges and domain gap induced by actions 'in the wild' videos.
		</li>
	      </ul>
	    </td>
	  </tr>
     </tbody></table>


    <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
	  <tr>
	    <td style="padding:20px;width:25%;vertical-align:middle">
	      <div class="one">
		<img src='https://github.com/skelemoa/ntu-x/raw/main/docs/images/NTU-X_sequence_diagram.png' width="160">
	      </div>
	    </td>
	    <td style="padding:20px;width:75%;vertical-align:middle">
	      <a href="https://skeleton.iiit.ac.in/ntux/">
		<papertitle>NTU-X: An Enhanced Large-scale Dataset for Improving Pose-based Recognition of Subtle Human Actions</papertitle>
	      </a>
	      <br>
	      <a href="https://researchweb.iiit.ac.in/~neel.trivedi/index.html">Neel Trivedi</a>,
	      <strong>Anirudh Thatipelli</strong>,
	      <a href="https://ravika.github.io/">Ravi Kiran Sarvadevabhatla</a>
	      <br>
	      <strong>[ORAL] 12th Indian Conference on Computer Vision, Graphics and Image Processing(ICVGIP, 2021)</strong>
	      <br>
	      <a href="https://arxiv.org/abs/2101.11529v3">paper</a> /
	      <a href="https://github.com/skelemoa/ntu-x">code</a>
	      <ul>
		<li>
		  <u>Description:</u>In addition to the 25 body joints for each skeleton as in NTU-RGBD, NTU60-X and NTU120-X dataset includes finger and facial joints, enabling a richer skeleton representation. We appropriately modify the state of the art approaches to enable training using the introduced datasets. Our results demonstrate the effectiveness of these NTU-X datasets in overcoming the aforementioned bottleneck and improve state of the art performance, overall and on previously worst performing action categories. 
		</li>
		<li>
		  <u>Outcome:</u> Our results demonstrate the effectiveness of these NTU-X datasets in overcoming the aforementioned bottleneck and improve state of the art performance, overall and on previously worst performing action categories.
		</li>
	      </ul>
	    </td>
	  </tr>
     </tbody></table>
</div><!-- /.blurb -->
